nohup: ignoring input
/home/ubuntu/miniforge3/envs/mm/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
PEMS07
Trainset:	x-(16921, 12, 883, 3)	y-(16921, 12, 883, 1)
Valset:  	x-(5640, 12, 883, 3)  	y-(5640, 12, 883, 1)
Testset:	x-(5640, 12, 883, 3)	y-(5640, 12, 883, 1)

--------- Cy2Mixer ---------
{
    "num_nodes": 883,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.6,
    "val_size": 0.2,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0.001,
    "milestones": [
        15,
        35,
        50
    ],
    "lr_decay_rate": 0.1,
    "batch_size": 16,
    "max_epochs": 300,
    "early_stop": 20,
    "use_cl": false,
    "cl_step_size": 2500,
    "model_args": {
        "gpu_num": 0,
        "num_nodes": 883,
        "in_steps": 12,
        "out_steps": 12,
        "steps_per_day": 288,
        "input_dim": 3,
        "output_dim": 1,
        "input_embedding_dim": 24,
        "tod_embedding_dim": 24,
        "dow_embedding_dim": 24,
        "spatial_embedding_dim": 0,
        "adaptive_embedding_dim": 80,
        "feed_forward_dim": 152,
        "num_layers": 4,
        "dropout": 0.4,
        "use_tinyatt": false,
        "dataset": "pems07"
    }
}
=========================================================================================================
Layer (type:depth-idx)                                  Output Shape              Param #
=========================================================================================================
Cy2Mixer                                                [16, 12, 883, 1]          847,680
├─Linear: 1-1                                           [16, 12, 883, 24]         96
├─Embedding: 1-2                                        [16, 12, 883, 24]         6,912
├─Embedding: 1-3                                        [16, 12, 883, 24]         168
├─ModuleList: 1-4                                       --                        --
│    └─Cy2Mixer_layer: 2-1                              [16, 12, 883, 152]        --
│    │    └─Cy2MixerBlock: 3-1                          [16, 883, 12, 152]        164,150
│    │    └─LayerNorm: 3-2                              [16, 12, 883, 152]        304
│    │    └─Dropout: 3-3                                [16, 12, 883, 152]        --
│    │    └─Cy2MixerBlock: 3-4                          [16, 12, 883, 152]        186,962
│    │    └─LayerNorm: 3-5                              [16, 12, 883, 152]        304
│    │    └─Dropout: 3-6                                [16, 12, 883, 152]        --
│    │    └─Cy2MixerBlock: 3-7                          [16, 12, 883, 152]        186,962
│    │    └─LayerNorm: 3-8                              [16, 12, 883, 152]        304
│    │    └─Dropout: 3-9                                [16, 12, 883, 152]        --
│    │    └─Conv2d: 3-10                                [16, 152, 883, 12]        69,464
│    └─Cy2Mixer_layer: 2-2                              [16, 12, 883, 152]        --
│    │    └─Cy2MixerBlock: 3-11                         [16, 883, 12, 152]        164,150
│    │    └─LayerNorm: 3-12                             [16, 12, 883, 152]        304
│    │    └─Dropout: 3-13                               [16, 12, 883, 152]        --
│    │    └─Cy2MixerBlock: 3-14                         [16, 12, 883, 152]        186,962
│    │    └─LayerNorm: 3-15                             [16, 12, 883, 152]        304
│    │    └─Dropout: 3-16                               [16, 12, 883, 152]        --
│    │    └─Cy2MixerBlock: 3-17                         [16, 12, 883, 152]        186,962
│    │    └─LayerNorm: 3-18                             [16, 12, 883, 152]        304
│    │    └─Dropout: 3-19                               [16, 12, 883, 152]        --
│    │    └─Conv2d: 3-20                                [16, 152, 883, 12]        69,464
│    └─Cy2Mixer_layer: 2-3                              [16, 12, 883, 152]        --
│    │    └─Cy2MixerBlock: 3-21                         [16, 883, 12, 152]        164,150
│    │    └─LayerNorm: 3-22                             [16, 12, 883, 152]        304
│    │    └─Dropout: 3-23                               [16, 12, 883, 152]        --
│    │    └─Cy2MixerBlock: 3-24                         [16, 12, 883, 152]        186,962
│    │    └─LayerNorm: 3-25                             [16, 12, 883, 152]        304
│    │    └─Dropout: 3-26                               [16, 12, 883, 152]        --
│    │    └─Cy2MixerBlock: 3-27                         [16, 12, 883, 152]        186,962
│    │    └─LayerNorm: 3-28                             [16, 12, 883, 152]        304
│    │    └─Dropout: 3-29                               [16, 12, 883, 152]        --
│    │    └─Conv2d: 3-30                                [16, 152, 883, 12]        69,464
│    └─Cy2Mixer_layer: 2-4                              [16, 12, 883, 152]        --
│    │    └─Cy2MixerBlock: 3-31                         [16, 883, 12, 152]        164,150
│    │    └─LayerNorm: 3-32                             [16, 12, 883, 152]        304
│    │    └─Dropout: 3-33                               [16, 12, 883, 152]        --
│    │    └─Cy2MixerBlock: 3-34                         [16, 12, 883, 152]        186,962
│    │    └─LayerNorm: 3-35                             [16, 12, 883, 152]        304
│    │    └─Dropout: 3-36                               [16, 12, 883, 152]        --
│    │    └─Cy2MixerBlock: 3-37                         [16, 12, 883, 152]        186,962
│    │    └─LayerNorm: 3-38                             [16, 12, 883, 152]        304
│    │    └─Dropout: 3-39                               [16, 12, 883, 152]        --
│    │    └─Conv2d: 3-40                                [16, 152, 883, 12]        69,464
├─Linear: 1-5                                           [16, 883, 12]             21,900
=========================================================================================================
Total params: 3,310,556
Trainable params: 3,310,556
Non-trainable params: 0
Total mult-adds (G): 50.94
=========================================================================================================
Input size (MB): 2.03
Forward/backward pass size (MB): 18273.27
Params size (MB): 5.38
Estimated Total Size (MB): 18280.68
=========================================================================================================

Loss: HuberLoss

2024-09-06 21:06:29.398187 Epoch 1  	Train Loss = 35.96668 Val Loss = 25.58809
2024-09-06 21:14:06.592655 Epoch 2  	Train Loss = 23.44591 Val Loss = 22.62872
2024-09-06 21:21:43.349143 Epoch 3  	Train Loss = 21.84736 Val Loss = 21.63183
2024-09-06 21:29:20.360409 Epoch 4  	Train Loss = 21.02980 Val Loss = 20.99713
2024-09-06 21:36:57.315739 Epoch 5  	Train Loss = 20.54959 Val Loss = 20.44121
2024-09-06 21:44:34.495673 Epoch 6  	Train Loss = 31.79681 Val Loss = 22.95204
2024-09-06 21:52:11.517907 Epoch 7  	Train Loss = 20.92259 Val Loss = 20.47612
2024-09-06 21:59:47.623256 Epoch 8  	Train Loss = 20.00426 Val Loss = 20.36895
2024-09-06 22:07:24.935373 Epoch 9  	Train Loss = 19.58851 Val Loss = 21.21010
2024-09-06 22:15:02.029272 Epoch 10  	Train Loss = 19.42615 Val Loss = 20.58119
2024-09-06 22:22:39.520298 Epoch 11  	Train Loss = 19.21109 Val Loss = 19.91478
2024-09-06 22:30:17.587984 Epoch 12  	Train Loss = 18.95203 Val Loss = 19.82735
2024-09-06 22:37:54.842349 Epoch 13  	Train Loss = 18.84539 Val Loss = 19.44590
2024-09-06 22:43:27.124530 Epoch 14  	Train Loss = 18.59698 Val Loss = 19.33372
2024-09-06 22:47:13.819360 Epoch 15  	Train Loss = 18.82138 Val Loss = 19.76092
2024-09-06 22:51:00.850840 Epoch 16  	Train Loss = 17.63872 Val Loss = 18.96282
2024-09-06 22:54:47.423363 Epoch 17  	Train Loss = 17.55243 Val Loss = 18.99611
2024-09-06 22:58:33.396647 Epoch 18  	Train Loss = 17.51191 Val Loss = 18.96310
2024-09-06 23:02:20.285077 Epoch 19  	Train Loss = 17.47111 Val Loss = 18.98109
2024-09-06 23:06:06.992825 Epoch 20  	Train Loss = 17.43458 Val Loss = 18.95977
2024-09-06 23:09:53.424195 Epoch 21  	Train Loss = 17.39672 Val Loss = 19.00561
2024-09-06 23:13:39.487790 Epoch 22  	Train Loss = 17.36647 Val Loss = 19.00244
2024-09-06 23:17:26.141623 Epoch 23  	Train Loss = 17.33049 Val Loss = 19.12937
2024-09-06 23:21:12.856988 Epoch 24  	Train Loss = 17.30163 Val Loss = 18.97240
2024-09-06 23:24:58.382160 Epoch 25  	Train Loss = 17.27228 Val Loss = 19.04152
2024-09-06 23:28:44.257164 Epoch 26  	Train Loss = 17.23920 Val Loss = 19.11476
2024-09-06 23:32:30.679212 Epoch 27  	Train Loss = 17.21451 Val Loss = 19.05718
2024-09-06 23:36:16.655576 Epoch 28  	Train Loss = 17.18436 Val Loss = 19.03377
2024-09-06 23:40:02.773809 Epoch 29  	Train Loss = 17.16063 Val Loss = 19.04043
2024-09-06 23:43:48.883357 Epoch 30  	Train Loss = 17.13032 Val Loss = 19.04570
2024-09-06 23:47:34.789154 Epoch 31  	Train Loss = 17.10829 Val Loss = 18.96634
2024-09-06 23:51:20.901324 Epoch 32  	Train Loss = 17.08179 Val Loss = 19.06095
2024-09-06 23:55:06.931491 Epoch 33  	Train Loss = 17.06120 Val Loss = 19.16628
2024-09-06 23:58:53.315212 Epoch 34  	Train Loss = 17.03843 Val Loss = 19.15238
2024-09-07 00:02:39.997907 Epoch 35  	Train Loss = 17.01384 Val Loss = 18.96350
2024-09-07 00:06:26.437416 Epoch 36  	Train Loss = 16.92313 Val Loss = 18.96673
2024-09-07 00:10:13.065963 Epoch 37  	Train Loss = 16.91128 Val Loss = 18.99177
2024-09-07 00:13:59.919737 Epoch 38  	Train Loss = 16.90632 Val Loss = 19.06429
2024-09-07 00:17:46.169578 Epoch 39  	Train Loss = 16.90403 Val Loss = 19.02892
2024-09-07 00:21:32.803986 Epoch 40  	Train Loss = 16.89877 Val Loss = 19.00303
Average Training Time : 277.58232272267344
Early stopping at epoch: 40
Best at epoch 20:
Train Loss = 17.43458
Train RMSE = 29.78492, MAE = 17.23471, MAPE = 7.51678
Val Loss = 18.95977
Val RMSE = 34.24717, MAE = 19.52077, MAPE = 8.47126
Saved Model: ../saved_models/Cy2Mixer-PEMS07-2024-09-06-20-58-48.pt
--------- Test ---------
All Steps RMSE = 33.97310, MAE = 19.51365, MAPE = 8.05915
Step 1 RMSE = 27.08137, MAE = 16.45628, MAPE = 6.86854
Step 2 RMSE = 29.43840, MAE = 17.45844, MAPE = 7.25470
Step 3 RMSE = 31.07107, MAE = 18.18777, MAPE = 7.52625
Step 4 RMSE = 32.32826, MAE = 18.75233, MAPE = 7.74144
Step 5 RMSE = 33.36478, MAE = 19.22555, MAPE = 7.92605
Step 6 RMSE = 34.24113, MAE = 19.63772, MAPE = 8.09043
Step 7 RMSE = 34.98943, MAE = 19.99795, MAPE = 8.22808
Step 8 RMSE = 35.62039, MAE = 20.31957, MAPE = 8.35131
Step 9 RMSE = 36.16518, MAE = 20.59817, MAPE = 8.47248
Step 10 RMSE = 36.67046, MAE = 20.86829, MAPE = 8.60527
Step 11 RMSE = 37.21040, MAE = 21.15988, MAPE = 8.73612
Step 12 RMSE = 37.72758, MAE = 21.49883, MAPE = 8.90792
Inference time: 20.27 s
