nohup: ignoring input
/home/ubuntu/miniforge3/envs/mm/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
PEMS08
Trainset:	x-(10700, 12, 170, 3)	y-(10700, 12, 170, 1)
Valset:  	x-(3567, 12, 170, 3)  	y-(3567, 12, 170, 1)
Testset:	x-(3566, 12, 170, 3)	y-(3566, 12, 170, 1)

--------- Cy2Mixer ---------
{
    "num_nodes": 170,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.6,
    "val_size": 0.2,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0.0015,
    "milestones": [
        25,
        45,
        65
    ],
    "lr_decay_rate": 0.1,
    "batch_size": 16,
    "max_epochs": 300,
    "early_stop": 30,
    "use_cl": false,
    "cl_step_size": 2500,
    "model_args": {
        "gpu_num": 0,
        "num_nodes": 170,
        "in_steps": 12,
        "out_steps": 12,
        "steps_per_day": 288,
        "input_dim": 3,
        "output_dim": 1,
        "input_embedding_dim": 24,
        "tod_embedding_dim": 24,
        "dow_embedding_dim": 24,
        "spatial_embedding_dim": 0,
        "adaptive_embedding_dim": 80,
        "feed_forward_dim": 152,
        "num_layers": 3,
        "dropout": 0.4,
        "use_tinyatt": true,
        "dataset": "pems08"
    }
}
=========================================================================================================
Layer (type:depth-idx)                                  Output Shape              Param #
=========================================================================================================
Cy2Mixer                                                [16, 12, 170, 1]          163,200
├─Linear: 1-1                                           [16, 12, 170, 24]         96
├─Embedding: 1-2                                        [16, 12, 170, 24]         6,912
├─Embedding: 1-3                                        [16, 12, 170, 24]         168
├─ModuleList: 1-4                                       --                        --
│    └─Cy2Mixer_layer: 2-1                              [16, 12, 170, 152]        --
│    │    └─Cy2MixerBlock: 3-1                          [16, 170, 12, 152]        164,150
│    │    └─LayerNorm: 3-2                              [16, 12, 170, 152]        304
│    │    └─Dropout: 3-3                                [16, 12, 170, 152]        --
│    │    └─Cy2MixerBlock: 3-4                          [16, 12, 170, 152]        186,962
│    │    └─LayerNorm: 3-5                              [16, 12, 170, 152]        304
│    │    └─Dropout: 3-6                                [16, 12, 170, 152]        --
│    │    └─Cy2MixerBlock: 3-7                          [16, 12, 170, 152]        186,962
│    │    └─LayerNorm: 3-8                              [16, 12, 170, 152]        304
│    │    └─Dropout: 3-9                                [16, 12, 170, 152]        --
│    │    └─Conv2d: 3-10                                [16, 152, 170, 12]        69,464
│    └─Cy2Mixer_layer: 2-2                              [16, 12, 170, 152]        --
│    │    └─Cy2MixerBlock: 3-11                         [16, 170, 12, 152]        164,150
│    │    └─LayerNorm: 3-12                             [16, 12, 170, 152]        304
│    │    └─Dropout: 3-13                               [16, 12, 170, 152]        --
│    │    └─Cy2MixerBlock: 3-14                         [16, 12, 170, 152]        186,962
│    │    └─LayerNorm: 3-15                             [16, 12, 170, 152]        304
│    │    └─Dropout: 3-16                               [16, 12, 170, 152]        --
│    │    └─Cy2MixerBlock: 3-17                         [16, 12, 170, 152]        186,962
│    │    └─LayerNorm: 3-18                             [16, 12, 170, 152]        304
│    │    └─Dropout: 3-19                               [16, 12, 170, 152]        --
│    │    └─Conv2d: 3-20                                [16, 152, 170, 12]        69,464
│    └─Cy2Mixer_layer: 2-3                              [16, 12, 170, 152]        --
│    │    └─Cy2MixerBlock: 3-21                         [16, 170, 12, 152]        164,150
│    │    └─LayerNorm: 3-22                             [16, 12, 170, 152]        304
│    │    └─Dropout: 3-23                               [16, 12, 170, 152]        --
│    │    └─Cy2MixerBlock: 3-24                         [16, 12, 170, 152]        186,962
│    │    └─LayerNorm: 3-25                             [16, 12, 170, 152]        304
│    │    └─Dropout: 3-26                               [16, 12, 170, 152]        --
│    │    └─Cy2MixerBlock: 3-27                         [16, 12, 170, 152]        186,962
│    │    └─LayerNorm: 3-28                             [16, 12, 170, 152]        304
│    │    └─Dropout: 3-29                               [16, 12, 170, 152]        --
│    │    └─Conv2d: 3-30                                [16, 152, 170, 12]        69,464
├─Linear: 1-5                                           [16, 170, 12]             21,900
=========================================================================================================
Total params: 2,017,626
Trainable params: 2,017,626
Non-trainable params: 0
Total mult-adds (G): 34.69
=========================================================================================================
Input size (MB): 0.39
Forward/backward pass size (MB): 4072.17
Params size (MB): 7.41
Estimated Total Size (MB): 4079.97
=========================================================================================================

Loss: HuberLoss

2024-09-10 04:50:29.719566 Epoch 1  	Train Loss = 28.93610 Val Loss = 20.63363
2024-09-10 04:52:14.522401 Epoch 2  	Train Loss = 18.48235 Val Loss = 17.83269
2024-09-10 04:53:59.407839 Epoch 3  	Train Loss = 16.99279 Val Loss = 15.96588
2024-09-10 04:55:35.055223 Epoch 4  	Train Loss = 16.25688 Val Loss = 16.16559
2024-09-10 04:57:11.214551 Epoch 5  	Train Loss = 15.58891 Val Loss = 15.63922
2024-09-10 04:58:56.504430 Epoch 6  	Train Loss = 15.17962 Val Loss = 15.01114
2024-09-10 05:00:37.498994 Epoch 7  	Train Loss = 14.74198 Val Loss = 15.04975
2024-09-10 05:02:17.470044 Epoch 8  	Train Loss = 14.43703 Val Loss = 14.33068
2024-09-10 05:03:53.139295 Epoch 9  	Train Loss = 14.23577 Val Loss = 14.02179
2024-09-10 05:05:29.259495 Epoch 10  	Train Loss = 13.86864 Val Loss = 14.00831
2024-09-10 05:07:10.675423 Epoch 11  	Train Loss = 13.76167 Val Loss = 14.15457
2024-09-10 05:08:54.188846 Epoch 12  	Train Loss = 13.52856 Val Loss = 13.92183
2024-09-10 05:10:36.135593 Epoch 13  	Train Loss = 13.40696 Val Loss = 13.82868
2024-09-10 05:12:19.794499 Epoch 14  	Train Loss = 13.31004 Val Loss = 14.74702
2024-09-10 05:14:01.509051 Epoch 15  	Train Loss = 13.09470 Val Loss = 13.81699
2024-09-10 05:15:43.998032 Epoch 16  	Train Loss = 12.96238 Val Loss = 13.63308
2024-09-10 05:17:25.536289 Epoch 17  	Train Loss = 12.90046 Val Loss = 13.66315
2024-09-10 05:19:04.888575 Epoch 18  	Train Loss = 12.82797 Val Loss = 13.56263
2024-09-10 05:20:49.609319 Epoch 19  	Train Loss = 12.69667 Val Loss = 13.66208
2024-09-10 05:22:25.504289 Epoch 20  	Train Loss = 12.70848 Val Loss = 13.68613
2024-09-10 05:24:05.421779 Epoch 21  	Train Loss = 12.55298 Val Loss = 13.66359
2024-09-10 05:25:41.165716 Epoch 22  	Train Loss = 12.47115 Val Loss = 13.93169
2024-09-10 05:27:24.946324 Epoch 23  	Train Loss = 12.45919 Val Loss = 13.66161
2024-09-10 05:29:08.313573 Epoch 24  	Train Loss = 12.30084 Val Loss = 14.04057
2024-09-10 05:30:51.964606 Epoch 25  	Train Loss = 12.21282 Val Loss = 13.61101
2024-09-10 05:32:35.724004 Epoch 26  	Train Loss = 11.62507 Val Loss = 13.34815
2024-09-10 05:34:19.155084 Epoch 27  	Train Loss = 11.48985 Val Loss = 13.34513
2024-09-10 05:36:03.519501 Epoch 28  	Train Loss = 11.44289 Val Loss = 13.41905
2024-09-10 05:37:46.497207 Epoch 29  	Train Loss = 11.40524 Val Loss = 13.38042
2024-09-10 05:39:29.027670 Epoch 30  	Train Loss = 11.37345 Val Loss = 13.44396
2024-09-10 05:41:07.742479 Epoch 31  	Train Loss = 11.34565 Val Loss = 13.43234
2024-09-10 05:42:52.273322 Epoch 32  	Train Loss = 11.31485 Val Loss = 13.51147
2024-09-10 05:44:35.653800 Epoch 33  	Train Loss = 11.28850 Val Loss = 13.47453
2024-09-10 05:46:16.451886 Epoch 34  	Train Loss = 11.26241 Val Loss = 13.50734
2024-09-10 05:47:56.649954 Epoch 35  	Train Loss = 11.23744 Val Loss = 13.49274
2024-09-10 05:49:38.065212 Epoch 36  	Train Loss = 11.21155 Val Loss = 13.51607
2024-09-10 05:51:14.648121 Epoch 37  	Train Loss = 11.18800 Val Loss = 13.52323
2024-09-10 05:52:53.195823 Epoch 38  	Train Loss = 11.16612 Val Loss = 13.55950
2024-09-10 05:54:35.319230 Epoch 39  	Train Loss = 11.14558 Val Loss = 13.57498
2024-09-10 05:56:10.103301 Epoch 40  	Train Loss = 11.12273 Val Loss = 13.56781
2024-09-10 05:57:49.278922 Epoch 41  	Train Loss = 11.10009 Val Loss = 13.62130
2024-09-10 05:59:28.643904 Epoch 42  	Train Loss = 11.08011 Val Loss = 13.59709
2024-09-10 06:01:05.375836 Epoch 43  	Train Loss = 11.05808 Val Loss = 13.62990
2024-09-10 06:02:48.381563 Epoch 44  	Train Loss = 11.04070 Val Loss = 13.68811
2024-09-10 06:04:33.073831 Epoch 45  	Train Loss = 11.01754 Val Loss = 13.67712
2024-09-10 06:06:20.208546 Epoch 46  	Train Loss = 10.95105 Val Loss = 13.63111
2024-09-10 06:08:04.293558 Epoch 47  	Train Loss = 10.93835 Val Loss = 13.61961
2024-09-10 06:09:48.815155 Epoch 48  	Train Loss = 10.93030 Val Loss = 13.64144
2024-09-10 06:11:31.966028 Epoch 49  	Train Loss = 10.92388 Val Loss = 13.63950
2024-09-10 06:13:12.344551 Epoch 50  	Train Loss = 10.92187 Val Loss = 13.65164
2024-09-10 06:14:57.048390 Epoch 51  	Train Loss = 10.91746 Val Loss = 13.65015
2024-09-10 06:16:39.787088 Epoch 52  	Train Loss = 10.91306 Val Loss = 13.63435
2024-09-10 06:18:22.191075 Epoch 53  	Train Loss = 10.91221 Val Loss = 13.64393
2024-09-10 06:20:06.260397 Epoch 54  	Train Loss = 10.90889 Val Loss = 13.64696
2024-09-10 06:21:46.510082 Epoch 55  	Train Loss = 10.90518 Val Loss = 13.65869
2024-09-10 06:23:29.064464 Epoch 56  	Train Loss = 10.90231 Val Loss = 13.66199
2024-09-10 06:25:13.365390 Epoch 57  	Train Loss = 10.90025 Val Loss = 13.65313
Average Training Time : 89.95432928152252
Early stopping at epoch: 57
Best at epoch 27:
Train Loss = 11.48985
Train RMSE = 19.50029, MAE = 11.10223, MAPE = 7.48911
Val Loss = 13.34513
Val RMSE = 26.24279, MAE = 14.07382, MAPE = 11.29440
Saved Model: ../saved_models/Cy2Mixer-PEMS08-2024-09-10-04-48-50.pt
--------- Test ---------
All Steps RMSE = 24.69475, MAE = 13.94209, MAPE = 9.09669
Step 1 RMSE = 19.87185, MAE = 11.80024, MAPE = 7.72875
Step 2 RMSE = 21.47691, MAE = 12.54326, MAPE = 8.17705
Step 3 RMSE = 22.54893, MAE = 13.01802, MAPE = 8.44396
Step 4 RMSE = 23.45200, MAE = 13.40814, MAPE = 8.70371
Step 5 RMSE = 24.22879, MAE = 13.74884, MAPE = 8.94182
Step 6 RMSE = 24.89916, MAE = 14.03758, MAPE = 9.10503
Step 7 RMSE = 25.44222, MAE = 14.28640, MAPE = 9.27921
Step 8 RMSE = 25.90949, MAE = 14.50902, MAPE = 9.43946
Step 9 RMSE = 26.33105, MAE = 14.71024, MAPE = 9.58299
Step 10 RMSE = 26.69180, MAE = 14.89035, MAPE = 9.74167
Step 11 RMSE = 26.98187, MAE = 15.06483, MAPE = 9.92528
Step 12 RMSE = 27.27563, MAE = 15.28824, MAPE = 10.09141
Inference time: 12.08 s
