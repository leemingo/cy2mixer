nohup: ignoring input
/home/ubuntu/miniforge3/envs/mm/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
PEMS08
Trainset:	x-(10700, 12, 170, 3)	y-(10700, 12, 170, 1)
Valset:  	x-(3567, 12, 170, 3)  	y-(3567, 12, 170, 1)
Testset:	x-(3566, 12, 170, 3)	y-(3566, 12, 170, 1)

--------- Cy2Mixer ---------
{
    "num_nodes": 170,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.6,
    "val_size": 0.2,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0.0015,
    "milestones": [
        25,
        45,
        65
    ],
    "lr_decay_rate": 0.1,
    "batch_size": 16,
    "max_epochs": 300,
    "early_stop": 30,
    "use_cl": false,
    "cl_step_size": 2500,
    "model_args": {
        "gpu_num": 0,
        "num_nodes": 170,
        "in_steps": 12,
        "out_steps": 12,
        "steps_per_day": 288,
        "input_dim": 3,
        "output_dim": 1,
        "input_embedding_dim": 24,
        "tod_embedding_dim": 24,
        "dow_embedding_dim": 24,
        "spatial_embedding_dim": 0,
        "adaptive_embedding_dim": 80,
        "feed_forward_dim": 152,
        "num_layers": 3,
        "dropout": 0.1,
        "use_tinyatt": true,
        "dataset": "pems08"
    }
}
=========================================================================================================
Layer (type:depth-idx)                                  Output Shape              Param #
=========================================================================================================
Cy2Mixer                                                [16, 12, 170, 1]          163,200
├─Linear: 1-1                                           [16, 12, 170, 24]         96
├─Embedding: 1-2                                        [16, 12, 170, 24]         6,912
├─Embedding: 1-3                                        [16, 12, 170, 24]         168
├─ModuleList: 1-4                                       --                        --
│    └─Cy2Mixer_layer: 2-1                              [16, 12, 170, 152]        --
│    │    └─Cy2MixerBlock: 3-1                          [16, 170, 12, 152]        164,150
│    │    └─LayerNorm: 3-2                              [16, 12, 170, 152]        304
│    │    └─Dropout: 3-3                                [16, 12, 170, 152]        --
│    │    └─Cy2MixerBlock: 3-4                          [16, 12, 170, 152]        186,962
│    │    └─LayerNorm: 3-5                              [16, 12, 170, 152]        304
│    │    └─Dropout: 3-6                                [16, 12, 170, 152]        --
│    │    └─Cy2MixerBlock: 3-7                          [16, 12, 170, 152]        186,962
│    │    └─LayerNorm: 3-8                              [16, 12, 170, 152]        304
│    │    └─Dropout: 3-9                                [16, 12, 170, 152]        --
│    │    └─Conv2d: 3-10                                [16, 152, 170, 12]        69,464
│    └─Cy2Mixer_layer: 2-2                              [16, 12, 170, 152]        --
│    │    └─Cy2MixerBlock: 3-11                         [16, 170, 12, 152]        164,150
│    │    └─LayerNorm: 3-12                             [16, 12, 170, 152]        304
│    │    └─Dropout: 3-13                               [16, 12, 170, 152]        --
│    │    └─Cy2MixerBlock: 3-14                         [16, 12, 170, 152]        186,962
│    │    └─LayerNorm: 3-15                             [16, 12, 170, 152]        304
│    │    └─Dropout: 3-16                               [16, 12, 170, 152]        --
│    │    └─Cy2MixerBlock: 3-17                         [16, 12, 170, 152]        186,962
│    │    └─LayerNorm: 3-18                             [16, 12, 170, 152]        304
│    │    └─Dropout: 3-19                               [16, 12, 170, 152]        --
│    │    └─Conv2d: 3-20                                [16, 152, 170, 12]        69,464
│    └─Cy2Mixer_layer: 2-3                              [16, 12, 170, 152]        --
│    │    └─Cy2MixerBlock: 3-21                         [16, 170, 12, 152]        164,150
│    │    └─LayerNorm: 3-22                             [16, 12, 170, 152]        304
│    │    └─Dropout: 3-23                               [16, 12, 170, 152]        --
│    │    └─Cy2MixerBlock: 3-24                         [16, 12, 170, 152]        186,962
│    │    └─LayerNorm: 3-25                             [16, 12, 170, 152]        304
│    │    └─Dropout: 3-26                               [16, 12, 170, 152]        --
│    │    └─Cy2MixerBlock: 3-27                         [16, 12, 170, 152]        186,962
│    │    └─LayerNorm: 3-28                             [16, 12, 170, 152]        304
│    │    └─Dropout: 3-29                               [16, 12, 170, 152]        --
│    │    └─Conv2d: 3-30                                [16, 152, 170, 12]        69,464
├─Linear: 1-5                                           [16, 170, 12]             21,900
=========================================================================================================
Total params: 2,017,626
Trainable params: 2,017,626
Non-trainable params: 0
Total mult-adds (G): 34.69
=========================================================================================================
Input size (MB): 0.39
Forward/backward pass size (MB): 4072.17
Params size (MB): 7.41
Estimated Total Size (MB): 4079.97
=========================================================================================================

Loss: HuberLoss

2024-09-06 21:00:48.633290 Epoch 1  	Train Loss = 28.48231 Val Loss = 19.22583
2024-09-06 21:02:38.619300 Epoch 2  	Train Loss = 18.69255 Val Loss = 17.01632
2024-09-06 21:04:28.683607 Epoch 3  	Train Loss = 16.71443 Val Loss = 16.02086
2024-09-06 21:06:15.145005 Epoch 4  	Train Loss = 16.17581 Val Loss = 15.44518
2024-09-06 21:08:03.620264 Epoch 5  	Train Loss = 15.58467 Val Loss = 16.02456
2024-09-06 21:09:53.830655 Epoch 6  	Train Loss = 15.10438 Val Loss = 14.86281
2024-09-06 21:11:43.723640 Epoch 7  	Train Loss = 14.62862 Val Loss = 14.50566
2024-09-06 21:13:32.751141 Epoch 8  	Train Loss = 14.65435 Val Loss = 15.00049
2024-09-06 21:15:18.666490 Epoch 9  	Train Loss = 14.20735 Val Loss = 14.16669
2024-09-06 21:17:08.708057 Epoch 10  	Train Loss = 14.04047 Val Loss = 14.75444
2024-09-06 21:18:58.730776 Epoch 11  	Train Loss = 13.77634 Val Loss = 13.92771
2024-09-06 21:20:48.665454 Epoch 12  	Train Loss = 13.50997 Val Loss = 14.10466
2024-09-06 21:22:33.664003 Epoch 13  	Train Loss = 13.36598 Val Loss = 14.07162
2024-09-06 21:24:23.684080 Epoch 14  	Train Loss = 13.38197 Val Loss = 14.21618
2024-09-06 21:26:13.531476 Epoch 15  	Train Loss = 13.19441 Val Loss = 13.93132
2024-09-06 21:28:03.581415 Epoch 16  	Train Loss = 13.00958 Val Loss = 13.84540
2024-09-06 21:29:48.599349 Epoch 17  	Train Loss = 12.89370 Val Loss = 13.60936
2024-09-06 21:31:38.711680 Epoch 18  	Train Loss = 12.83627 Val Loss = 13.74053
2024-09-06 21:33:28.894958 Epoch 19  	Train Loss = 12.67465 Val Loss = 13.80928
2024-09-06 21:35:18.927819 Epoch 20  	Train Loss = 12.62096 Val Loss = 13.87058
2024-09-06 21:37:03.273782 Epoch 21  	Train Loss = 12.57739 Val Loss = 13.81323
2024-09-06 21:38:53.243236 Epoch 22  	Train Loss = 12.35912 Val Loss = 13.97702
2024-09-06 21:40:43.351923 Epoch 23  	Train Loss = 12.29322 Val Loss = 13.77653
2024-09-06 21:42:33.462176 Epoch 24  	Train Loss = 12.33521 Val Loss = 13.70719
2024-09-06 21:44:19.760580 Epoch 25  	Train Loss = 12.12673 Val Loss = 13.60582
2024-09-06 21:46:07.754394 Epoch 26  	Train Loss = 11.44612 Val Loss = 13.41098
2024-09-06 21:47:57.591135 Epoch 27  	Train Loss = 11.30268 Val Loss = 13.44096
2024-09-06 21:49:47.571174 Epoch 28  	Train Loss = 11.24481 Val Loss = 13.47041
2024-09-06 21:51:36.649342 Epoch 29  	Train Loss = 11.20068 Val Loss = 13.48522
2024-09-06 21:53:22.611175 Epoch 30  	Train Loss = 11.16347 Val Loss = 13.53546
2024-09-06 21:55:12.878438 Epoch 31  	Train Loss = 11.12799 Val Loss = 13.54902
2024-09-06 21:57:03.399201 Epoch 32  	Train Loss = 11.09214 Val Loss = 13.59306
2024-09-06 21:58:53.440746 Epoch 33  	Train Loss = 11.05963 Val Loss = 13.58594
2024-09-06 22:00:38.068785 Epoch 34  	Train Loss = 11.02843 Val Loss = 13.61107
2024-09-06 22:02:28.297790 Epoch 35  	Train Loss = 11.00100 Val Loss = 13.63382
2024-09-06 22:04:18.472978 Epoch 36  	Train Loss = 10.97048 Val Loss = 13.63071
2024-09-06 22:06:08.667348 Epoch 37  	Train Loss = 10.94604 Val Loss = 13.65020
2024-09-06 22:07:53.297241 Epoch 38  	Train Loss = 10.92091 Val Loss = 13.69471
2024-09-06 22:09:43.413797 Epoch 39  	Train Loss = 10.89498 Val Loss = 13.69645
2024-09-06 22:11:33.355156 Epoch 40  	Train Loss = 10.87066 Val Loss = 13.72473
2024-09-06 22:13:23.244381 Epoch 41  	Train Loss = 10.84458 Val Loss = 13.75613
2024-09-06 22:15:07.511107 Epoch 42  	Train Loss = 10.82002 Val Loss = 13.73862
2024-09-06 22:16:57.343897 Epoch 43  	Train Loss = 10.79791 Val Loss = 13.75998
2024-09-06 22:18:47.258509 Epoch 44  	Train Loss = 10.77970 Val Loss = 13.80314
2024-09-06 22:20:37.357643 Epoch 45  	Train Loss = 10.75565 Val Loss = 13.79427
2024-09-06 22:22:23.741396 Epoch 46  	Train Loss = 10.68395 Val Loss = 13.77606
2024-09-06 22:24:11.643920 Epoch 47  	Train Loss = 10.66908 Val Loss = 13.77275
2024-09-06 22:26:01.695089 Epoch 48  	Train Loss = 10.66113 Val Loss = 13.78593
2024-09-06 22:27:51.870682 Epoch 49  	Train Loss = 10.65482 Val Loss = 13.77588
2024-09-06 22:29:41.009945 Epoch 50  	Train Loss = 10.65161 Val Loss = 13.77520
2024-09-06 22:31:26.409747 Epoch 51  	Train Loss = 10.64642 Val Loss = 13.78126
2024-09-06 22:33:16.431996 Epoch 52  	Train Loss = 10.64333 Val Loss = 13.78637
2024-09-06 22:35:06.211634 Epoch 53  	Train Loss = 10.63950 Val Loss = 13.78181
2024-09-06 22:36:55.974743 Epoch 54  	Train Loss = 10.63572 Val Loss = 13.79348
2024-09-06 22:38:41.065502 Epoch 55  	Train Loss = 10.63298 Val Loss = 13.79255
2024-09-06 22:40:31.111036 Epoch 56  	Train Loss = 10.62867 Val Loss = 13.79343
Average Training Time : 95.75247550010681
Early stopping at epoch: 56
Best at epoch 26:
Train Loss = 11.44612
Train RMSE = 18.96213, MAE = 10.81905, MAPE = 7.37709
Val Loss = 13.41098
Val RMSE = 25.92653, MAE = 14.22089, MAPE = 10.83073
Saved Model: ../saved_models/Cy2Mixer-PEMS08-2024-09-06-20-58-56.pt
--------- Test ---------
All Steps RMSE = 24.77456, MAE = 14.11973, MAPE = 9.15396
Step 1 RMSE = 20.15504, MAE = 12.01642, MAPE = 7.86604
Step 2 RMSE = 21.75760, MAE = 12.76018, MAPE = 8.29422
Step 3 RMSE = 22.82005, MAE = 13.23577, MAPE = 8.56569
Step 4 RMSE = 23.68769, MAE = 13.61959, MAPE = 8.80336
Step 5 RMSE = 24.40038, MAE = 13.93696, MAPE = 9.00424
Step 6 RMSE = 25.02499, MAE = 14.22941, MAPE = 9.18309
Step 7 RMSE = 25.53504, MAE = 14.47788, MAPE = 9.33811
Step 8 RMSE = 25.95414, MAE = 14.68986, MAPE = 9.48697
Step 9 RMSE = 26.29344, MAE = 14.86145, MAPE = 9.60870
Step 10 RMSE = 26.59366, MAE = 15.02434, MAPE = 9.75898
Step 11 RMSE = 26.88406, MAE = 15.18983, MAPE = 9.87951
Step 12 RMSE = 27.11570, MAE = 15.39505, MAPE = 10.05866
Inference time: 13.34 s
